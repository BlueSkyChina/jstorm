########### These all have default values as shown
########### Additional configuration goes into storm.yaml

java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"

### storm.* configs are general configurations
# the local dir is where jars are kept
storm.local.dir: "jstorm-local"
storm.zookeeper.servers:
    - "localhost"
storm.zookeeper.port: 2181
storm.zookeeper.root: "/jstorm"
storm.zookeeper.session.timeout: 20000
storm.zookeeper.connection.timeout: 15000
storm.zookeeper.retry.times: 5
storm.zookeeper.retry.interval: 1000
storm.zookeeper.retry.intervalceiling.millis: 30000
storm.cluster.mode: "distributed" # can be distributed or local
storm.local.mode.zmq: false
storm.thrift.transport: "backtype.storm.security.auth.SimpleTransportPlugin"
storm.messaging.transport: "com.alibaba.jstorm.message.zeroMq.MQContext"

### nimbus.* configs are for the master
nimbus.host: "localhost"
nimbus.thrift.port: 7627
nimbus.childopts: " -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 "
nimbus.task.timeout.secs: 30
nimbus.supervisor.timeout.secs: 60
nimbus.monitor.freq.secs: 10
nimbus.cleanup.inbox.freq.secs: 600
nimbus.inbox.jar.expiration.secs: 3600
nimbus.task.launch.secs: 240
nimbus.reassign: true
nimbus.file.copy.expiration.secs: 600
nimbus.topology.validator: "backtype.storm.nimbus.DefaultTopologyValidator"
nimbus.classpath: ""

### ui.* configs are for the master
ui.port: 8080
ui.childopts: " -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 "

drpc.port: 4772
drpc.worker.threads: 64
drpc.queue.size: 128
drpc.invocations.port: 4773
drpc.request.timeout.secs: 600
drpc.childopts: "-Xmx768m"

transactional.zookeeper.root: "/transactional"
transactional.zookeeper.servers: null
transactional.zookeeper.port: null

### supervisor.* configs are for node supervisors
# Define the amount of workers that can be run on this machine. Each worker is assigned a port to use for communication
supervisor.slots.ports:
    - 6800
    - 6801
    - 6802
    - 6803
supervisor.childopts: " -Xms256m -Xmx256m -XX:+UseConcMarkSweepGC -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 "
#how long supervisor will wait to ensure that a worker process is started
supervisor.worker.start.timeout.secs: 120
#how long between heartbeats until supervisor considers that worker dead and tries to restart it
supervisor.worker.timeout.secs: 30
#how frequently the supervisor checks on the status of the processes it's monitoring and restarts if necessary
supervisor.monitor.frequency.secs: 10
#how frequently the supervisor heartbeats to the cluster state (for nimbus)
supervisor.heartbeat.frequency.secs: 60
supervisor.enable: true

#How many cpu slot one supervisor can support , if it is null, it will be set by JStorm
supervisor.cpu.slot.num: null

#How many memory slot one supervisor can support , if it is null, it will be set by JStorm
supervisor.mem.slot.num: null

# How much disk slot one supervisor can support
# if it is null, it will use $(storm.local.dir)/worker_shared_data
supervisor.disk.slot: null
# if use multiple disks, it can be set as the following
#supervisor.disk.slot:
#	- /disk0/jstorm/data
#	- /disk1/jstorm/data
#	- /disk2/jstorm/data
#	- /disk3/jstorm/data

  

### worker.* configs are for task workers
# worker gc configuration
worker.gc.childopts: " -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 "
worker.heartbeat.frequency.secs: 2
worker.classpath: ""

task.heartbeat.frequency.secs: 10
task.refresh.poll.secs: 10

zmq.threads: 1
zmq.linger.millis: 5000
zmq.hwm: 0

storm.messaging.netty.server_worker_threads: 1
storm.messaging.netty.client_worker_threads: 1
storm.messaging.netty.buffer_size: 5242880 #5MB buffer
storm.messaging.netty.max_retries: 30
storm.messaging.netty.max_wait_ms: 1000
storm.messaging.netty.min_wait_ms: 100
storm.messaging.netty.disruptor: true

### topology.* configs are for specific executing storms
topology.enable.message.timeouts: true
topology.debug: false
topology.optimize: true
topology.workers: 1
topology.acker.executors: null
topology.tasks: null
# maximum amount of time a message has to complete before it's considered failed
topology.message.timeout.secs: 30
topology.skip.missing.kryo.registrations: false
topology.max.task.parallelism: null
topology.max.spout.pending: null
topology.state.synchronization.timeout.secs: 60
topology.stats.sample.rate: 0.05
topology.builtin.metrics.bucket.size.secs: 60
topology.fall.back.on.java.serialization: true
topology.worker.childopts: null
topology.executor.receive.buffer.size: 1024 #batched
topology.executor.send.buffer.size: 1024 #individual messages
topology.receiver.buffer.size: 8 # setting it too high causes a lot of problems (heartbeat thread gets starved, throughput plummets)
topology.transfer.buffer.size: 1024 # batched
topology.tick.tuple.freq.secs: null
topology.worker.shared.thread.pool.size: 4
topology.disruptor.wait.strategy: "com.lmax.disruptor.BlockingWaitStrategy"
topology.spout.wait.strategy: "backtype.storm.spout.SleepSpoutWaitStrategy"
topology.sleep.spout.wait.strategy.time.ms: 1
topology.error.throttle.interval.secs: 10
topology.max.error.report.per.interval: 5
topology.kryo.factory: "backtype.storm.serialization.DefaultKryoFactory"
topology.tuple.serializer: "backtype.storm.serialization.types.ListDelegateSerializer"
topology.trident.batch.emit.interval.millis: 500

# set topology resource assign priority weight
# the new topology resource assign resource will according to CPU/memory/disk/port 4 part
# so different weight, different priority
topology.disk.weight: 5
topology.cpu.weight: 3
topology.memory.weight: 1
topology.port.weight: 2

# if this is false, select supervisor whose weight is highest(left_disk_slot_num * disk.weight 
# + left_cpu_slot_num * cpu.weight + left_memory_slot_num * memory.weight + 
# left_port_slot_num * port.weight)
# it this is true, select which supervisor is best one according to slot level by level, 
# select the supervisor whose disk slot is the most
# if the last select result isn't 1, then select supervisor according to cpu
# then according to port, last  according to memory
topology.assign.supervisor.bylevel: true

#the following weight will decide which task will be assign firstly
topology.task.on.different.node.weight: 2
topology.use.old.assign.ratio.weight: 2
topology.user.define.assign.ratio.weight: 2

dev.zookeeper.path: "/tmp/dev-storm-zookeeper"

#if this configuration has been set, 
# the spout or bolt will log all received tuples
# topology.debug just for logging all sent tuples 
topology.debug.recv.tuple: false

#Usually, spout finish preparation before bolt, 
#so spout need wait several seconds so that bolt finish preparation
# the default setting is 30 seconds
spout.delay.run: 30

# One memory slot size, the unit size of memory, the default setting is 1G
# For example , if it is 1G, and one task set "memory.slots.per.task" as 2
# then the task will use 2G memory
memory.slot.per.size: 1073741824
